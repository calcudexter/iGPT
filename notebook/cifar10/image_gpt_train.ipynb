{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "igpt.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWrgLx2nsaW0"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
        "from google.colab import files,drive\n",
        "\n",
        "# Reduced dimension of our model\n",
        "n_px = 32\n",
        "n_embd = 128 \n",
        "n_head = 4\n",
        "n_layer = 8\n",
        "\n",
        "# Small iGPT dimensions\n",
        "# n_px = 32\n",
        "# n_embd = 512 \n",
        "# n_head = 8 \n",
        "# n_layer = 24 \n",
        "\n",
        "n_vocab = 513 # length of clusters + 1\n",
        "maxlen = n_px*n_px\n",
        "num_batch = 16\n",
        "num_epochs = 32\n",
        "\n",
        "color_clusters_dir = \"/content/clusters\"\n",
        "bs = 8 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U0GFglXRuHS"
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  \"\"\"Returns an upper triangular matrix filled with ones\"\"\"\n",
        "  mask = tf.linalg.band_part(tf.ones((size, size)), 0, -1)\n",
        " \n",
        "  return mask \n",
        "\n",
        "def attention(Q,K,V,mask):\n",
        "  \"\"\"Returns the masked attention weights\"\"\"\n",
        "  matmul_qk = tf.matmul(q,k,transpose_b = True)\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "  \n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "  output = tf.matmul(attention_weights, v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "def FullyConnected(embedding_dim, fully_connected_dim):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(fully_connected_dim, activation='gelu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "class Decoder_Layer(tf.keras.layers.Layer):\n",
        "    \n",
        "  def __init__(self, embedding_dim, num_heads, fully_connected_dim, layernorm_eps=1e-6):\n",
        "    super(Decoder_Layer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(num_heads=num_heads,\n",
        "                                  key_dim=embedding_dim) # key_dims is dk\n",
        "    self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                              fully_connected_dim=fully_connected_dim)\n",
        "    self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "    self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "  def call(self, x, look_ahead_mask):\n",
        "    \n",
        "    attn1, attn_weights_block1 = self.mha(x, x, x,look_ahead_mask, return_attention_scores=True)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "    out2 =  self.layernorm2(ffn_output + out1)\n",
        "\n",
        "    return out2, attn_weights_block1\n",
        "\n",
        "class Imagegpt(tf.keras.Model):\n",
        "   def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
        "               target_vocab_size, max_positional_encoding_input,\n",
        "               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n",
        "     \n",
        "     super(Imagegpt,self).__init__()\n",
        "\n",
        "     self.embedding_dim = embedding_dim\n",
        "     self.num_layers = num_layers\n",
        "     self.target_vocab_size = target_vocab_size\n",
        "     self.input_vocab_size = input_vocab_size\n",
        "\n",
        "     self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
        "     self.pos_encoding = Embedding(max_positional_encoding_input, self.embedding_dim)\n",
        "     self.decoder = Decoder_Layer(embedding_dim,num_heads,fully_connected_dim,layernorm_eps)\n",
        "\n",
        "   def call(self, inp):\n",
        "     x = self.embedding(inp)\n",
        "     x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n",
        "     x += self.pos_encoding(inp)\n",
        "\n",
        "     mask = create_look_ahead_mask(inp.shape[1])\n",
        "\n",
        "     for i in range(self.num_layers):\n",
        "       x, _ = self.decoder(x,mask)\n",
        "\n",
        "     return x,self.embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvlJCTOx7kaC"
      },
      "source": [
        "model = Imagegpt(num_layers=n_layer,\n",
        "                 embedding_dim=n_embd,\n",
        "                 num_heads=n_head,\n",
        "                 fully_connected_dim=n_embd*4,\n",
        "                 input_vocab_size=n_vocab,\n",
        "                 target_vocab_size=n_vocab,\n",
        "                 max_positional_encoding_input=maxlen,\n",
        "                 max_positional_encoding_target=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fwRazGqQUz8"
      },
      "source": [
        "import numpy as np\n",
        "color_clusters_file = \"%s/kmeans_centers.npy\"%(color_clusters_dir)\n",
        "clusters = np.load(color_clusters_file) #get color clusters\n",
        "\n",
        "#visualize samples with Image-GPT color palette.\n",
        "%matplotlib inline\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ37O9O9szmA"
      },
      "source": [
        "#numpy implementation of functions in image-gpt/src/utils which convert pixels of image to nearest color cluster. \n",
        "def normalize_img(img):\n",
        "  return img/127.5 - 1\n",
        "\n",
        "def squared_euclidean_distance_np(a,b):\n",
        "  b = b.T\n",
        "  a2 = np.sum(np.square(a),axis=1)\n",
        "  b2 = np.sum(np.square(b),axis=0)\n",
        "  ab = np.matmul(a,b)\n",
        "  d = a2[:,None] - 2*ab + b2[None,:]\n",
        "  return d\n",
        "\n",
        "def color_quantize_np(x, clusters):\n",
        "    x = x.reshape(-1, 3)\n",
        "    d = squared_euclidean_distance_np(x, clusters)\n",
        "    return np.argmin(d,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGMszyxbtA5z"
      },
      "source": [
        "#get images\n",
        "!curl https://i.imgur.com/fIiwqyn.jpeg > sg.jpeg\n",
        "image_paths = [\"sg.jpeg\"]*bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv3MupautBSZ"
      },
      "source": [
        "#Resize original images to n_px by n_px\n",
        "import cv2\n",
        "import numpy as np\n",
        "dim=(n_px,n_px)\n",
        "\n",
        "x = np.zeros((bs,n_px,n_px,3),dtype=np.uint8)\n",
        "\n",
        "for n,image_path in enumerate(image_paths):\n",
        "  img_np = cv2.imread(image_path)   # reads an image in the BGR format\n",
        "  img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)   # BGR -> RGB\n",
        "  H,W,C = img_np.shape\n",
        "  D = min(H,W)\n",
        "  img_np = img_np[:D,:D,:C] #get square piece of image\n",
        "  x[n] = cv2.resize(img_np,dim, interpolation = cv2.INTER_AREA) #resize to n_px by n_px"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRPMSWuatELU"
      },
      "source": [
        "#visualize resized images\n",
        "f, axes = plt.subplots(1,bs,dpi=300)\n",
        "\n",
        "for img,ax in zip(x,axes):\n",
        "    ax.axis('off')\n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YieXZLnxt8cg"
      },
      "source": [
        "#use Image-GPT color palette and crop images\n",
        "x_norm = normalize_img(x) #normalize pixels values to -1 to +1\n",
        "samples = color_quantize_np(x_norm,clusters).reshape(x_norm.shape[:-1]) #map pixels to closest color cluster\n",
        "\n",
        "n_px_crop = int(n_px/2)\n",
        "primers = samples.reshape(-1,n_px,n_px)[:,:n_px_crop,:n_px] # crop top n_px_crop rows. These will be the conditioning tokens\n",
        "\n",
        "context = samples.reshape(*samples.shape[:-2], -1)[:,:n_px_crop*n_px]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1IQZmqzUNP3"
      },
      "source": [
        "model.build(context.shape)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M344Wxe5qVaz"
      },
      "source": [
        "import cv2\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train[:100]\n",
        "x_train_norm = normalize_img(x_train)\n",
        "x_train_samples = color_quantize_np(x_train_norm,clusters).reshape(x_train_norm.shape[:-1])\n",
        "# print(x_train_samples.shape)\n",
        "\n",
        "samples_img = (np.rint(127.5 * (clusters[x_train_samples[:8]]+1.0))).astype(np.uint8)\n",
        "f, axes = plt.subplots(1,bs,dpi=300)\n",
        "for img,ax in zip(samples_img,axes):\n",
        "    ax.axis('off')\n",
        "    ax.imshow(img)\n",
        "\n",
        "x_train_samples = x_train_samples.reshape(*x_train_samples.shape[:-2], -1)\n",
        "y_train = x_train_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn6udKQKaCih"
      },
      "source": [
        "#visualize samples and crops with Image-GPT color palette. Should look similar to original resized images\n",
        "samples_img = (np.rint(127.5 * (clusters[samples]+1.0))).astype(np.uint8)\n",
        "primers_img = (np.rint(127.5 * (clusters[primers]+1.0))).astype(np.uint8) # convert color clusters back to pixels\n",
        "\n",
        "f, axes = plt.subplots(1,bs,dpi=300)\n",
        "for img,ax in zip(samples_img,axes):\n",
        "    ax.axis('off')\n",
        "    ax.imshow(img)\n",
        "\n",
        "f, axes2 = plt.subplots(1,bs,dpi=300)\n",
        "for img,ax in zip(primers_img,axes2):\n",
        "    ax.axis('off')\n",
        "    ax.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMuCGqQjZUQ"
      },
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3) \n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train = x_train[:10000]\n",
        "y_train = y_train[:10000]\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(num_batch)\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "      for epoch in range(num_epochs):\n",
        "          print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "          for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "              with tf.GradientTape() as tape:\n",
        "                    x_batch_train_norm = normalize_img(x_batch_train)\n",
        "                    x_batch_train_samples = color_quantize_np(x_batch_train_norm,clusters).reshape(x_batch_train_norm.shape[:-1])\n",
        "                    x_batch_train_samples = x_batch_train_samples.reshape(*x_batch_train_samples.shape[:-2], -1)\n",
        "                    \n",
        "                    h,emb_layer = model.call(x_batch_train_samples)\n",
        "                    wte = emb_layer.get_weights()\n",
        "                    h_flat = tf.reshape(h,[num_batch*maxlen,n_embd])\n",
        "                    gen_logits = tf.matmul(h_flat,wte,transpose_b = True)\n",
        "                    gen_logits = tf.reshape(gen_logits,[num_batch,maxlen,n_vocab])\n",
        "\n",
        "                    gen_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=gen_logits, labels=x_batch_train_samples)\n",
        "\n",
        "                    grads = tape.gradient(gen_losses,model.trainable_weights)\n",
        "                    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "                    # Log every 200 batches.\n",
        "                    if step % 200 == 0:\n",
        "                        print(\n",
        "                            \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                            % (step, float(tf.reduce_mean(gen_losses)))\n",
        "                        )\n",
        "                        print(\"Seen so far: %s samples\" % ((step + 1) * num_batch))\n",
        "            \n",
        "          model.save_weights(\"model_{}.h5\".format(epoch))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}